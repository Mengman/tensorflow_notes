{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import deepchem as dc\n",
    "\n",
    "np.random.seed(456)\n",
    "tf.set_random_seed(456)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/tox21.csv.gz\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "TIMING: featurizing shard 0 took 24.771 s\n",
      "TIMING: dataset construction took 25.196 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.880 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.619 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.918 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.934 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "_, (train, valid, test), _ = dc.molnet.load_tox21()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (6264, 1024) train_y shape: (6264, 12) train_w shape: (6264, 12)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w\n",
    "print(\"train_X shape: %s train_y shape: %s train_w shape: %s\" % (train_X.shape, train_y.shape, train_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (6264, 1024) train_y shape: (6264,) train_w shape: (6264,)\n"
     ]
    }
   ],
   "source": [
    "# Remove extra tasks\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "print(\"train_X shape: %s train_y shape: %s train_w shape: %s\" % (train_X.shape, train_y.shape, train_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1024\n",
    "n_hidden = 50\n",
    "learning_rate = .001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "  x = tf.placeholder(tf.float32, (None, d))\n",
    "  y = tf.placeholder(tf.float32, (None,))\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "  W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "  b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "  x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "with tf.name_scope(\"output\"):\n",
    "  W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "  b = tf.Variable(tf.random_normal((1,)))\n",
    "  y_logit = tf.matmul(x_hidden, W) + b\n",
    "  # the sigmoid gives the class probability of 1\n",
    "  y_one_prob = tf.sigmoid(y_logit)\n",
    "  # Rounding P(y=1) will give the correct prediction.\n",
    "  y_pred = tf.round(y_one_prob)\n",
    "with tf.name_scope(\"loss\"):\n",
    "  # Compute the cross-entropy term for each datapoint\n",
    "  y_expand = tf.expand_dims(y, 1)\n",
    "  entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "  # Sum all contributions\n",
    "  l = tf.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "  train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "  tf.summary.scalar(\"loss\", l)\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/tmp/fcnet-tox21',\n",
    "                                     tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 129.205490\n",
      "epoch 1, loss: 116.366806\n",
      "epoch 2, loss: 108.215279\n",
      "epoch 3, loss: 100.721436\n",
      "epoch 4, loss: 94.093353\n",
      "epoch 5, loss: 87.907280\n",
      "epoch 6, loss: 81.931938\n",
      "epoch 7, loss: 75.996826\n",
      "epoch 8, loss: 70.318352\n",
      "epoch 9, loss: 65.336685\n",
      "Unweighted Classification Accuracy: 0.932312\n",
      "Weighted Classification Accuracy: 0.644972\n"
     ]
    }
   ],
   "source": [
    "N = train_X.shape[0]\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  step = 0\n",
    "  for epoch in range(n_epochs):\n",
    "    pos = 0\n",
    "    while pos < N:\n",
    "      batch_X = train_X[pos:pos+batch_size]\n",
    "      batch_y = train_y[pos:pos+batch_size]\n",
    "      feed_dict = {x: batch_X, y: batch_y}\n",
    "      _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "      \n",
    "      train_writer.add_summary(summary, step)\n",
    "      step += 1\n",
    "      pos += batch_size\n",
    "    print(\"epoch %d, loss: %f\" % (epoch, loss))\n",
    "  # Make Predictions\n",
    "  valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})\n",
    "\n",
    "score = accuracy_score(valid_y, valid_y_pred)\n",
    "print(\"Unweighted Classification Accuracy: %f\" % score)\n",
    "\n",
    "weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Weighted Classification Accuracy: %f\" % weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
