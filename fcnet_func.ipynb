{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import tox21\n",
    "\n",
    "tf.set_random_seed(456)\n",
    "np.random.seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tox21_hyperparams(n_hidden=50, n_layers=1, learning_rate=.001,\n",
    "                           dropout_prob=0.5, n_epochs=45, batch_size=100,\n",
    "                           weight_positives=True):\n",
    "    d = 1024\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        train_X, train_y, train_w, valid_X, valid_y, valid_w, test_X, test_y, test_w = tox21()\n",
    "        \n",
    "        with tf.name_scope(\"placeholders\"):\n",
    "            x = tf.placeholder(tf.float32, (None, d))\n",
    "            y = tf.placeholder(tf.float32, (None,))\n",
    "            w = tf.placeholder(tf.float32, (None,))\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            with tf.name_scope(\"layer-%d\" % layer):\n",
    "                W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "                b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "                x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "                x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "                \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "            b = tf.Variable(tf.random_normal((1,)))\n",
    "            y_logit = tf.matmul(x_hidden, W) + b\n",
    "            \n",
    "            y_one_prob = tf.sigmoid(y_logit)\n",
    "            y_pred = tf.round(y_one_prob)\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            y_expand = tf.expand_dims(y, 1)\n",
    "            entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "            \n",
    "            if weight_positives:\n",
    "                w_expand = tf.expand_dims(w, 1)\n",
    "                entropy = w_expand * entropy\n",
    "            \n",
    "            l = tf.reduce_sum(entropy)\n",
    "            \n",
    "        with tf.name_scope(\"optim\"):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "        \n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", l)\n",
    "            merged = tf.summary.merge_all()\n",
    "        \n",
    "        hyperparam_str = \"d-%d-hidden-%d-lr-%f-n_epochs-%d-batch_size-%d-weight_pos-%s\" % (d, n_hidden, learning_rate, n_epochs, batch_size, str(weight_positives))\n",
    "        train_writer = tf.summary.FileWriter('/tmp/fcnet-func-' + hyperparam_str, tf.get_default_graph())\n",
    "        \n",
    "        N = train_X.shape[0]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            step = 0\n",
    "            for epoch in range(n_epochs):\n",
    "                pos = 0\n",
    "                while pos < N:\n",
    "                    batch_X = train_X[pos:pos+batch_size]\n",
    "                    batch_y = train_y[pos:pos+batch_size]\n",
    "                    batch_w = train_w[pos:pos+batch_size]\n",
    "                    feed_dict = {x: batch_X, y: batch_y, w: batch_w, keep_prob: dropout_prob}\n",
    "                    _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "                    train_writer.add_summary(summary, step)\n",
    "                    \n",
    "                    step += 1\n",
    "                    pos += batch_size\n",
    "#                 print(\"epoch %d, loss: %f\" % (epoch, loss))\n",
    "            valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X, keep_prob: 1.0})\n",
    "        weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "        print(\"Valid Weighted Classification Accuracy: %f\" % weighted_score)\n",
    "    return weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "epoch 0, loss: 2282.731934\n",
      "epoch 1, loss: 1385.962891\n",
      "epoch 2, loss: 1038.810791\n",
      "epoch 3, loss: 1908.433838\n",
      "epoch 4, loss: 1469.491333\n",
      "epoch 5, loss: 1610.777100\n",
      "epoch 6, loss: 834.324951\n",
      "epoch 7, loss: 1091.189819\n",
      "epoch 8, loss: 325.707184\n",
      "epoch 9, loss: 1064.686401\n",
      "epoch 10, loss: 1773.819214\n",
      "epoch 11, loss: 794.721863\n",
      "epoch 12, loss: 948.814697\n",
      "epoch 13, loss: 1453.650391\n",
      "epoch 14, loss: 1228.517334\n",
      "epoch 15, loss: 968.245972\n",
      "epoch 16, loss: 1711.731445\n",
      "epoch 17, loss: 467.960327\n",
      "epoch 18, loss: 951.676758\n",
      "epoch 19, loss: 194.095566\n",
      "epoch 20, loss: 335.381775\n",
      "epoch 21, loss: 986.657471\n",
      "epoch 22, loss: 1066.617798\n",
      "epoch 23, loss: 320.587158\n",
      "epoch 24, loss: 242.460190\n",
      "epoch 25, loss: 252.873444\n",
      "epoch 26, loss: 764.278625\n",
      "epoch 27, loss: 388.613831\n",
      "epoch 28, loss: 754.788208\n",
      "epoch 29, loss: 420.059998\n",
      "epoch 30, loss: 588.575806\n",
      "epoch 31, loss: 519.417725\n",
      "epoch 32, loss: 39.453426\n",
      "epoch 33, loss: 648.704651\n",
      "epoch 34, loss: 508.225342\n",
      "epoch 35, loss: 282.555542\n",
      "epoch 36, loss: 95.115601\n",
      "epoch 37, loss: 84.319908\n",
      "epoch 38, loss: 628.057861\n",
      "epoch 39, loss: 100.867432\n",
      "epoch 40, loss: 54.849194\n",
      "epoch 41, loss: 80.048874\n",
      "epoch 42, loss: 471.145813\n",
      "epoch 43, loss: 34.845600\n",
      "epoch 44, loss: 542.315857\n",
      "Valid Weighted Classification Accuracy: 0.646833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64683275113462824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tox21_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "n_reps = 3\n",
    "hidden_sizes = [30, 60]\n",
    "epochs = [15, 30, 45]\n",
    "dropouts = [.5]\n",
    "num_layers = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.594250\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.577952\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.624987\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.633515\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.659444\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.625727\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.646092\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.657221\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.642388\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.673158\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.675363\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.652759\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.637976\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.669832\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.617578\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.630551\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.623505\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.636840\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.649434\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.623867\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.664251\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.649072\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.654982\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.667214\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.609808\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.619439\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.638338\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.659823\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.667972\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.611635\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.653138\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.675742\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.665732\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.637202\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.616442\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Valid Weighted Classification Accuracy: 0.638305\n",
      "All Scores\n",
      "{(30, 15, 0.5, 1): [0.59425025833319345, 0.63797629807934364, 0.60980779051395717], (30, 15, 0.5, 2): [0.57795189128667912, 0.66983219730662169, 0.61943864376871571], (60, 15, 0.5, 1): [0.62498651392841476, 0.61757816527090814, 0.63833832417878822], (60, 15, 0.5, 2): [0.63351450621797811, 0.63055116675497547, 0.65982253528555712], (30, 30, 0.5, 1): [0.65944372651925087, 0.62350484419691343, 0.66797171880881423], (30, 30, 0.5, 2): [0.62572734879416536, 0.63683987178042512, 0.61163470367804129], (60, 30, 0.5, 1): [0.64609191626887752, 0.6494340644981863, 0.65313823882693955], (60, 30, 0.5, 2): [0.65722122192199894, 0.62386687029635801, 0.67574209356576531], (30, 45, 0.5, 1): [0.64238774194012427, 0.66425076181319931, 0.66573243154470063], (30, 45, 0.5, 2): [0.6731575628690688, 0.64907203839874172, 0.63720189787986969], (60, 45, 0.5, 1): [0.67536328479945906, 0.65498193465788535, 0.61644173897198973], (60, 45, 0.5, 2): [0.65275943006063342, 0.66721410127620195, 0.63830475884506488]}\n"
     ]
    }
   ],
   "source": [
    "for rep in range(n_reps):\n",
    "    for n_epochs in epochs:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for dropout in dropouts:\n",
    "                for n_layers in num_layers:\n",
    "                    score = eval_tox21_hyperparams(n_hidden=hidden_size, n_epochs=n_epochs,\n",
    "                                         dropout_prob=dropout, n_layers=n_layers)\n",
    "                    if (hidden_size, n_epochs, dropout, n_layers) not in scores:\n",
    "                        scores[(hidden_size, n_epochs, dropout, n_layers)] = []\n",
    "                    scores[(hidden_size, n_epochs, dropout, n_layers)].append(score)\n",
    "print(\"All Scores\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Averaged over 3 repetitions\n",
      "{(30, 15, 0.5, 1): 0.61401144897549809, (30, 15, 0.5, 2): 0.62240757745400555, (60, 15, 0.5, 1): 0.62696766779270374, (60, 15, 0.5, 2): 0.64129606941950357, (30, 30, 0.5, 1): 0.65030676317499292, (30, 30, 0.5, 2): 0.62473397475087722, (60, 30, 0.5, 1): 0.64955473986466783, (60, 30, 0.5, 2): 0.65227672859470742, (30, 45, 0.5, 1): 0.6574569784326747, (30, 45, 0.5, 2): 0.65314383304922674, (60, 45, 0.5, 1): 0.64892898614311134, (60, 45, 0.5, 2): 0.65275943006063342}\n"
     ]
    }
   ],
   "source": [
    "avg_scores = {}\n",
    "for params, param_scores in scores.items():\n",
    "  avg_scores[params] = np.mean(np.array(param_scores))\n",
    "print(\"Scores Averaged over %d repetitions\" % n_reps)\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
